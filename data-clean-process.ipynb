{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcKeI3FiXjTU"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from geopy import distance\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7i5SNwUXjTW"
   },
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv(\"kaggle_data/train.csv\")\n",
    "print(f'Size before removing missing data: {len(raw_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing data and duplicates\n",
    "my_dataset = raw_train[raw_train['MISSING_DATA'] == False]\n",
    "my_dataset = my_dataset[my_dataset['POLYLINE'] != '[]']\n",
    "\n",
    "progress_bar = tqdm(total=len(my_dataset))\n",
    "def polylength(polyline):\n",
    "    progress_bar.update(1)\n",
    "    poly = eval(polyline[\"POLYLINE\"])\n",
    "\n",
    "    if len(poly) == 1:\n",
    "        return -1.0, True, -1.0, -1.0, -1.0, -1.0\n",
    "    \n",
    "    # KM/HR\n",
    "    def speed(x):\n",
    "        coord1, coord2 = x\n",
    "        return distance.geodesic((coord1[1], coord1[0]), (coord2[1], coord2[0])).km\n",
    "    distance_list = list(map(speed, zip(poly, poly[1:])))\n",
    "    speed_list = np.array(distance_list, dtype=float)*240\n",
    "        \n",
    "    # Travel Time & Speed Exceeds 200 KM/HR\n",
    "    return (len(poly)-1)*15, (max(speed_list) >= 200), poly[0], poly[-1], sum(distance_list), np.average(speed_list)\n",
    "\n",
    "my_dataset[[\"TRAVEL_TIME\", \"IRREGULAR\", \"START\", \"END\", \"LENGTH\", \"AVG_SPEED\"]] = my_dataset[[\"POLYLINE\"]].apply(polylength, axis=1, result_type=\"expand\")\n",
    "progress_bar.close()\n",
    "my_dataset = my_dataset.drop('POLYLINE', axis=1)\n",
    "\n",
    "my_dataset = my_dataset.reset_index()\n",
    "my_dataset = my_dataset.drop(['MISSING_DATA', 'index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_dataset.to_csv('gps_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = pd.read_csv('gps_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = my_dataset[my_dataset['IRREGULAR'] != True]\n",
    "my_dataset = my_dataset.drop('IRREGULAR', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size after removing missing data: {len(my_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Iy0nQjpXjTZ"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import holidays\n",
    "\n",
    "portugal_holidays = holidays.country_holidays('PT', subdiv='Ext')\n",
    "\n",
    "def parse_time(x):\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  dt_tuple = dt.timetuple()\n",
    "  holiday = 0\n",
    "  if dt.date() in portugal_holidays:\n",
    "    holiday = 2\n",
    "  elif dt.date() + timedelta(days=1) in portugal_holidays:\n",
    "    holiday = 1\n",
    "  return (dt.hour*4 + dt.minute//15), dt.weekday(), dt.date().isocalendar().week - 1, holiday\n",
    "\n",
    "my_dataset[[\"QTRHR\", \"WK\", \"WKYR\", \"HOLIDAY\"]] = my_dataset[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "my_dataset = my_dataset.drop('DAY_TYPE', axis=1)\n",
    "my_dataset = my_dataset.drop('TIMESTAMP', axis=1)\n",
    "\n",
    "my_dataset['ORIGIN_CALL'] = my_dataset['ORIGIN_CALL'].fillna(0)\n",
    "my_dataset['ORIGIN_STAND'] = my_dataset['ORIGIN_STAND'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OC = sorted(my_dataset['ORIGIN_CALL'].astype(int).unique())\n",
    "label_encoder.fit(OC)\n",
    "my_dataset['ORIGIN_CALL'] = label_encoder.transform(my_dataset['ORIGIN_CALL'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(range(64))\n",
    "my_dataset['ORIGIN_STAND'] = label_encoder.transform(my_dataset['ORIGIN_STAND'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TI = sorted(my_dataset['TAXI_ID'].astype(int).unique())\n",
    "TI.insert(0, 0)\n",
    "\n",
    "label_encoder.fit(TI)\n",
    "my_dataset['TAXI_ID'] = label_encoder.transform(my_dataset['TAXI_ID'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.to_csv('processed_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('kaggle_data/test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[[\"QTRHR\", \"WK\", \"WKYR\", \"HOLIDAY\"]] = test_df[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "test_df = test_df.drop(['MISSING_DATA', 'DAY_TYPE', 'TIMESTAMP'], axis=1)\n",
    "test_df['ORIGIN_CALL'] = test_df['ORIGIN_CALL'].fillna(0)\n",
    "test_df['ORIGIN_STAND'] = test_df['ORIGIN_STAND'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(OC)\n",
    "test_df['ORIGIN_CALL'] = test_df['ORIGIN_CALL'].apply(lambda x: 0 if x not in label_encoder.classes_ else x)\n",
    "test_df['ORIGIN_CALL'] = label_encoder.transform(test_df['ORIGIN_CALL'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(range(64))\n",
    "test_df['ORIGIN_STAND'] = label_encoder.transform(test_df['ORIGIN_STAND'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.fit(TI)\n",
    "test_df['TAXI_ID'] = test_df['TAXI_ID'].apply(lambda x: 0 if x not in label_encoder.classes_ else x)\n",
    "test_df['TAXI_ID'] = label_encoder.transform(test_df['TAXI_ID'].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('processed_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
